---
title: "Linear Regression"
author: "Weijia Wang"
date: "2/16/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Multiple Linear Regression Model

```{r }
happy <- read.csv(file = '~/Downloads/2019.csv', header = TRUE)
```


We want to see the relationship between two at a time. Notice that Life Expectancy and GDP per Capita have a strong positive relationship, which is 0.84. It means the higher Life Expectancy, the higher GDP per Capita.
```{r }
happy <- as.data.frame(happy[,c( 'Score', 'GDP.per.capita', 'Social.support', 'Healthy.life.expectancy', 'Freedom.to.make.life.choices', 'Generosity')])
cor(happy)
```

Then, we fit a linear model with 4 predictors and 1 response using lm() function. 
```{r }
fit <- lm(Score ~ GDP.per.capita + Social.support + Healthy.life.expectancy + Freedom.to.make.life.choices + Generosity , data=happy)
summ_fit <- summary(fit)
summary(fit)
```

We are using F-test, and our null hypothesis is $H_0: \beta_1 = \beta_2 = ... = \beta_5 = 0$ and the alternative hypothesis is $H_a: \beta_j \neq 0$ for at least one $j \in \{1, 2, ..., 5\}$. Since the p-value is less than 2e-16, we can reject our null hypothesis. We also see the $R^2$ value, which is the coefficient of determination. It shows the goodness of fit in a model, which 0 presents a poor fit while 1 presents a good fit. In this case, the $R^2$ equals 0.774, which means that 77.4% of the variables are explained by the model and is not a poor fit. We can see that GDP per Capita, Social Support, and Life Expectancy are significant for the coefficients since the p-values are less than 0.05. The regression coefficients explain the Happiness Score expected to increase for a unit change in a predictor variable, controlling all other predictor variables constant.

Then we find the confidence intervals of each coefficient using confint() function. Take an example of GPD per Capita, [0.41, 1.3] is a 95% confidence interval for the true change in the Happiness Score for a 1% change in GDP per Capita. 
```{r }
confint(fit)
```

Then, we need to each each assumption. The first assumption is normality. Shapiro-Wilk Test and qq-plot can check if the random errors are normally distributed with mean zero. Since the p-value is 0.1, which cannot reject null hypothesis, the residuals have a normal distribution and the variables are satisfied with the assumption. 
```{r }
library(car)
shapiro.test(summ_fit$residuals)
qqPlot(summ_fit$residuals)
residplot <- function(fit, nbreaks=20) {
  z <- rstudent(fit)
  hist(z, breaks=nbreaks, freq=FALSE, xlab="Studentized Residual",main="Distribution of Errors")
  rug(jitter(z), col="brown")
  curve(dnorm(x, mean=mean(z), sd=sd(z)), add=TRUE, col="blue", lwd=1) 
  lines(density(z)$x, density(z)$y, col="red", lwd=2, lty=2) 
  legend("topright",legend = c( "Normal Curve", "Kernel Density Curve"),lty=1:2, col=c("blue","red"), cex=.7)
 }
residplot(fit)
```

To check whether the random errors are independence, we can use scatterplot of predictor vs. residuals. Most of the points lie in between -2 and 2. Thus, the assumptions of equal values and independence are satisfied. 
```{r }
z <- rstudent(fit)
par(mfrow = c(2,3))
plot(happy$GDP.per.capita, z) 
abline(h=0,col=2,lwd=2) 
abline(h=2, col=3,lwd=2, lty=2) 
abline(h=-2,col=3,lwd=2, lty=2)

plot(happy$Social.support, z) 
abline(h=0,col=2,lwd=2) 
abline(h=2, col=3,lwd=2, lty=2) 
abline(h=-2,col=3,lwd=2, lty=2)

plot(happy$Healthy.life.expectancy, z) 
abline(h=0,col=2,lwd=2) 
abline(h=2, col=3,lwd=2, lty=2) 
abline(h=-2,col=3,lwd=2, lty=2)

plot(happy$Freedom.to.make.life.choices, z) 
abline(h=0,col=2,lwd=2) 
abline(h=2, col=3,lwd=2, lty=2) 
abline(h=-2,col=3,lwd=2, lty=2)

plot(happy$Generosity, z) 
abline(h=0,col=2,lwd=2) 
abline(h=2, col=3,lwd=2, lty=2) 
abline(h=-2,col=3,lwd=2, lty=2)

plot(fit$fitted.values, z) 
abline(h=0,col=2,lwd=2) 
abline(h=2,col=3,lwd=2, lty=2) 
abline(h=-2,col=3,lwd=2, lty=2)
```

Then, using Cook's distance to check is there any data is influential. There are three observations are influential. 
```{r }
cutoff <- 4/(nrow(state.x77)-length(fit$coefficients)) 
plot(fit, which=4, cook.levels=cutoff) 
abline(h=cutoff, lty=2, col="red")
```

There is one outlier, Botswana, that isn't predicted well based on the linear model. The model over estimate the Happiness Score. The actual Happiness Score for Botswana should be 3.5, but the model predict a 5.2. 
```{r }
outlierTest(fit)
df[148,]
fitted(fit)[148]
```

To check the dependent variable is linear related to the independent variables, we can use the "partial residual plot". From the graphs, we can see there is evidence of linearity. 
```{r }
avPlots(fit, ask=FALSE)
```

To check homoscedasticity, we want to see the variance of the random errors doesn't vary with the level of independence variables. Non-constant Variance Score Test can be used to identify the non-constant error variance. Since the p-value is 0.3, we cannot reject the null hypothesis. Thus, the homoscedasticity assumption is satisfied. 
```{r}
library(car) 
ncvTest(fit)
```

Since we found that Generosity is not significant in this model, we want to compare the two fit nested models. Model 1 is nested in model 2. Since the p-value is 0.13, we can conclude that we don't need to add Generosity to the predictor. 
```{r }
fit <- lm(Score ~ GDP.per.capita + Social.support + Healthy.life.expectancy + Freedom.to.make.life.choices + Generosity , data=happy)
fit2 <- lm(Score ~ GDP.per.capita + Social.support + Healthy.life.expectancy + Freedom.to.make.life.choices , data=happy)
anova(fit2, fit)
```

This graph indicates that a one-standard-deviation increase in GDP per Capita yields a 0.84 standard deviation increase with other predictors holding. We can see that Freedom to Make Life Choices is the most important predictor and Generosity is the least. 
```{r }
zstates <- as.data.frame(scale(happy))
zfit <- lm(Score ~ GDP.per.capita + Social.support + Healthy.life.expectancy + Freedom.to.make.life.choices + Generosity , data=happy)
coef(zfit)
plot(zfit$coefficients,col='dark blue', ylab = 'Regression Coefficients',
     pch=19,cex = 2,xlab = '')
grid()
text(seq(1, 6, by=1),par("usr")[3] -0.1,labels = (names(zfit$coefficients)),
     srt = 45, pos = 1, xpd = TRUE)
```
